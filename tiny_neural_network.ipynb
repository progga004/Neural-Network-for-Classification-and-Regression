{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b39e1f8-9ad9-4f02-b1bd-b6c34fc3333a",
   "metadata": {},
   "source": [
    "In this notebook we will learn the basic building blocks of PyTorch, and implement a small, simple neural network, but using the flexible and expandable framework.\n",
    "\n",
    "As you did with the decision tree, the recommended process is to first work through the notebook, and then copy/paste the methods into the accompanying tinyNeuralNet.py, which you can then test using github classroom. Then, when you have passed all the tests, you can create the figures asked for in the homework writeup.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af51c6f5-7c4a-4d01-ac3a-32d6ef4e614e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "\n",
    "SMALLNUMBER = 0.00000001\n",
    "BIGNUMBER = 100000000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0faf9072-01d5-4e79-9c84-a96045c468c8",
   "metadata": {},
   "source": [
    "Since we will need it many times, first code up the sigmoid function. You may want to test this against github classroom first, to avoid any early errors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d28bf61-c5e5-4887-9e7f-272dbd035422",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_fn(s):\n",
    "   return 1/(1+np.exp(-s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8227a5d5-3d6c-4e80-9ea5-5efb0e8544a2",
   "metadata": {},
   "source": [
    "To understand the structure of this code, I have written up the main parts of the class Model() for you, so you do not need to modify it. Study the format, and use it to guide the building of the remaining compoents. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a7c227b-a72b-4d2d-8b08-c4bca40070fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Model():\n",
    "    def __init__(self):\n",
    "        self.states = [] # contains state variables s_k\n",
    "        self.states_grad = [] # contains derivatives d loss / d s_k. Note that d Loss/ ds_0 is not needed.\n",
    "        self.layers = [] #contains network modules\n",
    "        \n",
    "    def forward(self,X,y):\n",
    "        s = X\n",
    "        self.states = [s]\n",
    "        for l in self.layers:\n",
    "            s = l.forward(s)\n",
    "            self.states.append(s)\n",
    "        loss = self.loss_fn.forward(s,y)\n",
    "        self.loss = loss\n",
    "        \n",
    "        yhat = s\n",
    "        return np.mean(loss), yhat\n",
    "        \n",
    "    \n",
    "    def backward(self,X,y):\n",
    "        sgrad = self.loss_fn.backward(y,self.states[-1])\n",
    "        self.states_grad = [sgrad]\n",
    "        for i in range(len(self.states)-2,0,-1):\n",
    "            sgrad = self.layers[i].backward(sgrad,self.states[i])\n",
    "            self.states_grad.insert(0,sgrad)\n",
    "            \n",
    "            \n",
    "    def update_params(self, stepsize):\n",
    "        for i in range(len(self.layers)):\n",
    "            layer = self.layers[i]\n",
    "            if layer.num_params > 0:\n",
    "                layer.update_var(self.states_grad[i],self.states[i], stepsize)\n",
    "                \n",
    "                \n",
    "                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a058924-ef8e-4983-9b85-efcfa5fadd6a",
   "metadata": {},
   "source": [
    "Next, we will build the  two activation functions (ReLU and Sigmoid). You do not need to touch the Module class, but fill in the forward and backward functions in ReLU and Sigmoid. \n",
    "\n",
    "Remember, a forward function performs s_next = f(s), and a backward function computes d loss / d s, given d loss / d splus, and s. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3082e0f8-c615-4f99-a616-e44da79d3f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Module():\n",
    "    def __init__(self):\n",
    "        self.params = None\n",
    "    \n",
    "    \n",
    "class ReLU(Module):\n",
    "    def __init__(self):\n",
    "        self.num_params = 0.\n",
    "        \n",
    "    def forward(self,s):\n",
    "       return np.maximum(0,s)\n",
    "    \n",
    "    \n",
    "    def backward(self,dLdsn,s):\n",
    "        dL_ds_in = dLdsn.copy()\n",
    "        dL_ds_in[s <= 0] = 0\n",
    "        return dL_ds_in\n",
    "       \n",
    "    \n",
    "    \n",
    "class Sigmoid(Module):\n",
    "    def __init__(self):\n",
    "        self.num_params = 0.\n",
    "        \n",
    "    def forward(self,s):\n",
    "        return 1/(1+np.exp(-s))\n",
    "    \n",
    "    \n",
    "    def backward(self,dLdsn,s):\n",
    "        sig = self.forward(s)\n",
    "        dsig = sig * (1 - sig)  \n",
    "        return dLdsn * dsig\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7393f4-5a7d-4606-afc5-85d3af2ac6f7",
   "metadata": {},
   "source": [
    "The linear module is similar to the two activation functions, in that it requires a forward and backward step. However, additionally, you also need to have a function that produces d loss / d W and d loss / d b, in order to do the back propagation. Then, using the given step size, make gradient steps. Test these functions carefully before moving on.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2555dcae-3270-4735-910a-25567decf9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Linear(Module):\n",
    "    def __init__(self, num_outparam,num_inparam, weight_std = 1):\n",
    "        self.W = np.random.randn(num_outparam,num_inparam)*weight_std\n",
    "        self.b = np.random.randn(num_outparam)*weight_std\n",
    "        self.num_params = num_inparam*num_outparam + num_outparam\n",
    "        \n",
    "    def forward(self,s):\n",
    "         self.output = np.dot(self.W, s) + np.outer(self.b, np.ones(s.shape[1]))\n",
    "         return self.output\n",
    "    def backward(self,dLdsn,s):\n",
    "       self.dW = np.dot(dLdsn, s.T)  # Shape should be the same as self.W\n",
    "\n",
    "        # Gradient with respect to biases\n",
    "       self.db = np.sum(dLdsn, axis=1)  # Sum across samples if batch processing\n",
    "\n",
    "        # Gradient with respect to input\n",
    "       grad_input = np.dot(self.W.T, dLdsn)\n",
    "       return grad_input\n",
    "    \n",
    "    \n",
    "    def update_var(self,dLdsn,s, stepsize):\n",
    "       dW = 0.\n",
    "       db = 0.\n",
    "        \n",
    "   \n",
    "       batchsize = s.shape[1]\n",
    "       dW = np.dot(dLdsn,s.T)\n",
    "       db = np.sum(dLdsn,axis=1)\n",
    "        \n",
    "\n",
    "\n",
    "       self.W = self.W - dW*stepsize\n",
    "       self.b = self.b - db*stepsize\n",
    "        \n",
    "       return self.W,self.b\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dc9b99-ec4d-4b2a-ba3f-4b703d53d860",
   "metadata": {},
   "source": [
    "In the next section, we will focus on making a simple neural network, using a ReLU activation function, and a BCEloss (which is Pytorch's name for logistic loss). The loss function is much like the modules, except in its forward and backward steps, it is focusing on acting on y and yhat.  Code up this loss.\n",
    "\n",
    "Note that to deal with numerical issues, we will use a SMALLNUMBER in the log function to avoid getting infs. Since the output of this loss function does not affect the back propagation, we can rest easy to know that this trick only helps us with our metrics, but does not affect the guts of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "610d2df8-4d16-496c-b342-9fd8df4790da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Loss():\n",
    "    pass\n",
    "\n",
    "class BCELoss(Loss):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    # f(y,yhat) = -log(max(SMALLNUMBER, sigmoid(y*yhat)))\n",
    "    def forward(self,y,yhat):\n",
    "       yhat_clipped = np.clip(yhat, SMALLNUMBER, 1 - SMALLNUMBER)  # Clip yhat to avoid log(0)\n",
    "       loss = -np.mean(y * np.log(yhat_clipped) + (1 - y) * np.log(1 - yhat_clipped))\n",
    "       return loss\n",
    "        \n",
    "    def backward(self,y,yhat):\n",
    "        yhat_clipped = np.clip(yhat, SMALLNUMBER, 1 - SMALLNUMBER)  # Clip yhat to avoid division by zero\n",
    "        grad = (yhat_clipped - y) / (yhat_clipped * (1 - yhat_clipped))\n",
    "        return grad\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb0015c-f921-4bdf-a90f-07a1b44e07c4",
   "metadata": {},
   "source": [
    "Finally, build the neural network. You will do this by filling self.layers with the necessary units. The rest of the work is now handled by the Model parent class!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fe28e4a-eb01-470d-a307-2df83d26cef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "class SimpleReluClassNN(Model):\n",
    "    def __init__(self, num_layers, hidden_width, num_inparam):\n",
    "       super().__init__()\n",
    "        # Create layers\n",
    "       for i in range(num_layers):\n",
    "            input_dim = num_inparam if i == 0 else hidden_width\n",
    "            output_dim = 1 if i == num_layers - 1 else hidden_width\n",
    "\n",
    "            # Add a fully connected layer\n",
    "            self.layers.append(Linear(input_dim, output_dim))\n",
    "\n",
    "            # Add a ReLU layer, except for the output layer\n",
    "            if i < num_layers - 1:\n",
    "                self.layers.append(ReLU())\n",
    "    \n",
    "        \n",
    "    def inference(self,X,y):\n",
    "        loss, yhat = self.forward(X,y)\n",
    "        yhat = np.sign(yhat)\n",
    "        return loss, yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92396c0-632a-4aa8-a2a0-401838d0511f",
   "metadata": {},
   "source": [
    "Test your overall neural network. Plot its train/test loss, and train/test accuracy over 0/1 disambiguation, over a range of step sizes, width, and layers. You pick what to sweep, but pick at least 2 interesting hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "943636b9-3a29-4740-8394-5167b7f90f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sio.loadmat('mnist.mat')\n",
    "Xtrain = data['trainX'].T\n",
    "Xtest = data['testX'].T\n",
    "ytrain = data['trainY'][0,:]\n",
    "ytest = data['testY'][0,:]\n",
    "\n",
    "\n",
    "idx = np.less(ytrain,2)\n",
    "Xtrain = Xtrain[:,idx]\n",
    "ytrain = ytrain[idx].astype(int)\n",
    "ytrain[ytrain==0] = -1\n",
    "\n",
    "idx = np.less(ytest,2)\n",
    "Xtest = Xtest[:,idx]\n",
    "ytest = ytest[idx].astype(int)\n",
    "ytest[ytest==0] = -1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f00816-9f5f-4434-af78-00f56ddbe82d",
   "metadata": {},
   "source": [
    "Well, that was fun, wasn't it? And you can see how this whole construct is flexible and extendable? \n",
    "\n",
    "To show you really understand the concept, now build a simple neural network, but instead of ReLU, use a sigmoid function as an activation, and instead of BCEloss, use MSEloss (e.g. regression loss). Use the numeric labels as the target value. Again, report the train/test loss, over a couple of interesting hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42a2eeed-91c0-479f-8a02-69ea6ca98713",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class MSELoss(Loss):\n",
    "    def __init__(self):\n",
    "        self.num_params = 0.\n",
    "        \n",
    "    def forward(self,y,yhat):\n",
    "        return None\n",
    "        \n",
    "    def backward(self,y,yhat):\n",
    "        return None\n",
    "    \n",
    "    \n",
    "class SimpleSigmoidRegressNN(Model):\n",
    "    def __init__(self, num_layers, hidden_width, num_inparam):\n",
    "        pass\n",
    "        \n",
    "    def inference(self,X,y):\n",
    "        loss, yhat = self.forward(X,y)        \n",
    "        return loss, yhat\n",
    "        \n",
    "        \n",
    "        \n",
    "###################\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "data = sio.loadmat('mnist.mat')\n",
    "Xtrain = data['trainX'].T\n",
    "Xtest = data['testX'].T\n",
    "ytrain = data['trainY'][0,:].astype(float)\n",
    "ytest = data['testY'][0,:].astype(float)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d4b25b-3ba2-4900-8334-1940cc915931",
   "metadata": {},
   "source": [
    "Finally, show that you can extend this concept to *multiclass* regression as well. Note that here, yhat is not a 1-D variable, but rather a k-D variable, where k is the number of classes. \n",
    "\n",
    "Hint: Start by coding up CrossEntropyLoss and testing that correctness first. \n",
    "\n",
    "Report the train/test loss, and train/test accuracy over iterations, across 2 interesting hyperparameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3f056a3-f7d9-4947-a247-4669e1bf5509",
   "metadata": {},
   "outputs": [],
   "source": [
    "## multiclass classification\n",
    "\n",
    "    \n",
    "\n",
    "class CrossEntropyLoss(Loss):\n",
    "    def __init__(self):\n",
    "        self.num_params = 0.\n",
    "        \n",
    "    def forward(self,y,yhat):\n",
    "        return None\n",
    "        \n",
    "    def backward(self,y,yhat):\n",
    "        return None\n",
    "    \n",
    "    \n",
    "class SimpleReluMulticlassNN(Model):\n",
    "    def __init__(self, num_layers, hidden_width, num_inparam, num_classes):\n",
    "        pass\n",
    "    \n",
    "    def inference(self,X,y):\n",
    "        loss, yhat = self.forward(X,y)\n",
    "        yhat = np.argmax(yhat,axis=0)\n",
    "        \n",
    "        return loss, yhat\n",
    "\n",
    "##################\n",
    "\n",
    "    \n",
    "import scipy\n",
    "    \n",
    "\n",
    "\n",
    "data = sio.loadmat('mnist.mat')\n",
    "Xtrain = data['trainX'].T\n",
    "Xtest = data['testX'].T\n",
    "ytrain = data['trainY'][0,:]\n",
    "ytest = data['testY'][0,:]\n",
    "\n",
    "ytrain_mat = scipy.sparse.coo_matrix((np.ones(len(ytrain)),(ytrain,range(len(ytrain))))).toarray()\n",
    "ytest_mat = scipy.sparse.coo_matrix((np.ones(len(ytest)),(ytest,range(len(ytest))))).toarray()\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f15f2ea-9248-411d-8bfe-41ef6ee132d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
